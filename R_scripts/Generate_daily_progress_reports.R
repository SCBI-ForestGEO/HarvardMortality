# Generate plots of error rates over time and other error rate analyses for 
# paper as seen here
# https://github.com/SCBI-ForestGEO/continuous_integration/issues/14
library(dplyr)
library(readxl)
library(readr)
library(stringr)
library(here)
library(janitor)
library(lubridate)
library(patchwork)
library(tidyr)
library(ggplot2)



# Load quadrat info and censused stem counts --------------------------------
# Get latest FFF
latest_FFFs <- list.files(here("raw_data/FFF_excel/"), pattern = ".xlsx", full.names = T)

# Get info on all quadrats censused so far
quadrat_info <- read_xlsx(latest_FFFs, sheet = "Root", .name_repair = "minimal" ) %>% 
  clean_names() %>% 
  select(submission_id, quad, date_time, personnel, quadrat_stem_count) %>% 
  mutate(
    quadrat_stem_count = as.numeric(quadrat_stem_count),
    date_time = ymd(date_time)
  )

# Get number of stems censused per day
stems_censused_per_day <- quadrat_info %>% 
  group_by(date_time) %>% 
  summarize(n_stems = sum(quadrat_stem_count))


# Load trace of error reports ---------------------------------------
# Field fix errors per day
field_fix_errors <- read_csv("testthat/reports/trace_of_reports/require_field_fix_error_file.csv") %>% 
  clean_names() %>% 
  distinct() %>% 
  select(error_name, surveyor_id, submission_id, date_time = orig_collection_date, quad_sub_quad, stem_tag) %>% 
  mutate(
    quad = str_sub(quad_sub_quad, 1, 4),
    date_time = ymd(date_time)
  ) %>% 
  select(-quad_sub_quad) %>% 
  # NOTE: A large number of erroneous "duplicated_stem" errors (~1300)
  # appear to have been accidentally manually appended to the
  # cummulative log of errors
  # trace_of_reports/require_field_fix_error_file.csv in these commits:
  # https://github.com/SCBI-ForestGEO/HarvardMortality/commit/58105fc24009b7d699c1db492693ace4d57c81c0
  # https://github.com/SCBI-ForestGEO/HarvardMortality/commit/ec7630c3d23cb3cc0e0f980fffb230799fae3902
  # 
  # However I'm confident this was an error because the corresponding
  # daily field fix errors report automatically generated by GitHub
  # Actions did not include these "duplicated_stem" errors:
  # https://github.com/SCBI-ForestGEO/HarvardMortality/commit/ec83b37fcf582fb0d574f52df0ae9f5177bc6f9e
  #
  # Hence I'm manually filtering them out:
  filter(date_time != ymd("2021-07-15") & error_name != "duplicated_stem") %>% 
  # Determine level of aggregation:
  group_by(date_time) %>% 
  summarize(field_fix = n()) 

# Auto fixes
auto_fix_errors <- read_csv("testthat/reports/trace_of_reports/will_auto_fix_error_file.csv") %>% 
  clean_names() %>% 
  distinct() %>% 
  select(error_name, surveyor_id, submission_id, date_time = orig_collection_date, quad_sub_quad, stem_tag) %>% 
  mutate(
    quad = str_sub(quad_sub_quad, 1, 4),
    date_time = ymd(date_time)
  ) %>% 
  select(-quad_sub_quad) %>% 
  # Determine level of aggregation:
  group_by(date_time) %>% 
  summarize(auto_fix = n()) 

# Missing stems
quadrat_censused_missing_stems <- read_csv("testthat/reports/trace_of_reports/quadrat_censused_missing_stems.csv") %>% 
  clean_names() %>% 
  distinct() %>% 
  mutate(
    quad = str_pad(quadrat, 4, "left", "0")
  ) %>% 
  select(-quadrat) %>% 
  left_join(quadrat_info %>% select(date_time, quad, personnel), by = "quad") %>% 
  # Determine level of aggregation:
  group_by(date_time) %>% 
  summarize(missing_stems = n()) 

# Duplicated stems
quadrat_censused_duplicated_stems <- read_csv("testthat/reports/trace_of_reports/quadrat_censused_duplicated_stems.csv") %>% 
  clean_names() %>% 
  select(surveyor_id, submission_id, date_time = orig_collection_date, quad_sub_quad, stem_tag) %>% 
  mutate(
    quad = str_sub(quad_sub_quad, 1, 4),
    date_time = ymd(date_time)
  ) %>% 
  select(-quad_sub_quad) %>% 
  # Count any duplicates only once
  group_by(submission_id, date_time, quad, stem_tag) %>% 
  slice(1) %>% 
  # Determine level of aggregation:
  group_by(date_time) %>% 
  summarize(duplicated_stems = n())

# NOTE: After 2021-07-11 duplicated_stems errors were merged into 
# field_fix_error report
field_fix_errors <-
  quadrat_censused_duplicated_stems %>% 
  rename(field_fix = duplicated_stems) %>% 
  bind_rows(field_fix_errors) %>% 
  group_by(date_time) %>% 
  summarize(field_fix = sum(field_fix))


# Merge all data ---------------------------------------
error_rates <- 
  # Merge error reports
  field_fix_errors %>% 
  left_join(auto_fix_errors, by = "date_time") %>% 
  left_join(quadrat_censused_missing_stems, by = "date_time") %>% 
  replace(is.na(.), 0) %>%
  pivot_longer(cols = -c(date_time), names_to = "error_type", values_to = "n_errors") %>% 
  # Get error rate being sure to account for days where 0 errors occurred
  full_join(stems_censused_per_day, by = "date_time") %>% 
  mutate(errors_per_stem = n_errors/n_stems) %>% 
  complete(date_time, error_type) %>% 
  filter(!is.na(error_type)) %>% 
  replace(is.na(.), 0)



# Generate all plots ---------------------------------------
census_rate_plot <- stems_censused_per_day %>% 
  ggplot(aes(x=date_time, y = n_stems)) + 
  geom_point() +
  geom_line() + 
  labs(
    x = "Census date", y = "# of new stems censused",
    title = "Daily new stems censused"
  )

error_rate_plot <- error_rates %>%
  ggplot(aes(x=date_time, y = errors_per_stem, col = error_type)) +
  geom_point() +
  geom_line() + 
  labs(
    x = "Census date", y = "# of errors per stem censused",
    col = "Error type",
    title = "Daily (new) error rate"
  ) + 
  coord_cartesian(
    ylim = c(0, NA)
  ) +
  theme(legend.position="bottom")

# Merge ggplots using patchwork
progress_plot <- census_rate_plot /
  error_rate_plot 

# Output
filename <- file.path(here("testthat"), "reports/daily_progress.png")
ggsave(filename, plot= progress_plot, device = 'png', width = 16/2, height = 18/2.5, units = "in",dpi = 300 )





